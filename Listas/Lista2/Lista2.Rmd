---
title: "Lista 2"
author: "Aline - RA236032, Nathan - RA236258, Thiago - RA194340, Vanessa - RA204383"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(reticulate)
library(JuliaCall)

julia_setup(JULIA_HOME = "C:/Users/vahfe/AppData/Local/Programs/Julia-1.10.4/bin")
use_python("C:/Users/vahfe/AppData/Local/Programs/Python/Python311/python.exe", 
           required = TRUE)
```


1.  Seja o modelo de regressão linear múltipla. Prove que

a)  $\hat{\beta} = ({X^\prime X})^{-1}({X^\prime Y})$

O estimador para $\beta$ por mínimos quadrados é aquele que minimiza a soma dos quadrados dos erros $\epsilon_i$.

A soma do quadrados dos erros, no modelo de regressão linear simples, pode ser definida por:

$$
Q = \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n (Y_i - \beta_0 - \beta_1X_i)^2
$$

E em notação matricial, temos:

$$
Q = \sum_{i=1}^n \epsilon_i^2 = \epsilon^\prime \epsilon 
= ({Y} - {X}\beta)^\prime ({Y} - {X}\beta) 
= ({Y}^\prime - \beta^\prime {X}^\prime) ({Y} - {X}\beta) 
= {Y}^\prime {Y} - {Y}^\prime {X}\beta - \beta^\prime {X}^\prime {Y} + \beta^\prime {X}^\prime {X}\beta
$$

Mas $\beta^\prime {X}^\prime {Y}$ é um escalar, isto é, tem dimensão $1 \times 1$ e por isso é igual ao seu transposto: $\beta^\prime {X}^\prime {Y} = {Y}^\prime {X} \beta$.

Então, chegamos na expressão:

$$
Q(\beta)= {Y}^\prime {Y} - 2{X}^\prime\beta^\prime {Y} + {X}^\prime\beta^\prime {X} \beta
$$

Se queremos o estimador para $\beta$ por mínimos quadrados precisamo encontrar os pontos de mínimo de $Q$. Para isso vamos derivar $Q$ em relação a $\beta$ e igualar a zero.

$$
\frac{\partial Q}{\partial \beta} = -2{X}^\prime{Y} + 2{X}^\prime{X}\beta = 0 \Rightarrow {X}^\prime{X}\hat{\beta} = {X}^\prime{Y}
$$

Assim, se ${X}^\prime{X}$ tiver posto completo, sua inversa existe e chegamos na solução:

$$
\hat{\beta} = ({X}^\prime{X})^{-1} {X}^\prime{Y}
$$

b)  $\mathbb{E}(\hat{\beta}) = \beta$

Por suposição, sabemos que $\mathbb{E}(\epsilon) = {0}$ e $\mathbb{V}(\epsilon) = \sigma^2 {I}$.

Além disso, veja que podemos escrever

$$
{\hat{\beta}} = (X^\prime X)^{-1} X^\prime Y
= (X^\prime X)^{-1} X^\prime (X \beta + \epsilon)
= (X^\prime X)^{-1} X^\prime X \beta + (X^\prime X)^{-1} X^\prime \epsilon
= \beta + (X^\prime X)^{-1} X^\prime \epsilon
$$

Portanto,

$$
\mathbb{E}({\hat{\beta}}) = \mathbb{E}(\beta + (X^\prime X)^{-1} X^\prime \epsilon)
= \beta + (X^\prime X)^{-1} X^\prime \mathbb{E}(\epsilon)
= \beta + (X^\prime X)^{-1} X^\prime \cdot0
= \beta
$$

c)  $\mathbb{V}(\hat{\beta}|{X}) = \sigma^2({X^\prime X})^{-1}$

Temos que 

$$
\mathbb{V}(\hat{\beta}) = \mathbb{V}[({X}^\prime {X})^{-1}{X}^\prime {Y}]
$$

Mas sabemos que se ${Y}$ é um vetor aleatório e ${A}$ é uma matriz fixa (não aleatória), então a variância do vetor transformado ${AY}$ é dada por $\mathbb{V}(A {Y}) = {A} \mathbb{V}({Y}) {A}^\prime$. Então:

$$
\mathbb{V}[({X}^\prime {X})^{-1}{X}^\prime {Y}]
= ({X}^\prime {X})^{-1}{X}^\prime \mathbb{V}({Y}) {X} [({X}^\prime {X})^{-1}]^\prime
$$

Mas lembre-se que $[({X}^\prime {X})^{-1}]^\prime = [({X}^\prime {X})^\prime]^{-1}$ e que, como ${X}^\prime {X}$ é simétrica, isto é, $({X}^\prime {X})^\prime = {X}^\prime {X}$, temos que $[({X}^\prime {X})^\prime]^{-1} = ({X}^\prime {X})^{-1}$. Logo:

$$
({X}^\prime {X})^{-1}{X}^\prime \mathbb{V}({Y}) {X} [({X}^\prime {X})^{-1}]^\prime 
=
({X}^\prime {X})^{-1}{X}^\prime \mathbb{V}({Y}) {X} ({X}^\prime {X})^{-1} 
$$
$$
=
({X}^\prime {X})^{-1}{X}^\prime \sigma^2 {I} {X} ({X}^\prime {X})^{-1}
=
\sigma^2 ({X}^\prime {X})^{-1}{X}^\prime{X} ({X}^\prime {X})^{-1}
=
\sigma^2 ({X}^\prime {X})^{-1}
$$

2.  Mostre que $\mathbb{E}(\hat{u}|{X}) = 0$

Sabendo que $\hat{u} = Y-\hat Y$, temos que

$$
\mathbb{E}(\hat{u}|{X}) = \mathbb{E}(Y-\hat Y|{X}) 
= \mathbb{E}(Y|{X}) - \mathbb{E}(\hat Y|{X}) 
$$

Em que o primeiro termo pode ser escrito como

$$
\mathbb{E}(Y|{X}) = \mathbb{E}({X}\beta + u | {X} )
= \mathbb{E}({X}\beta|{X}) + \mathbb{E}(u|{X})
$$

Mas como ${X}$ e $\beta$ são conhecidos e, por suposição do modelo de regressão linear, $\mathbb{E}(u|{X}) = 0$, temos que 

$$
\mathbb{E}(Y|{X}) = {X}\beta + 0 = {X}\beta
$$

Já o segundo termo pode ser escrito como

$$
\mathbb{E}(\hat Y|{X}) = \mathbb{E}({X}\hat\beta|{X})
$$

em que $\hat\beta$ é aleatório, mas ${X}$ não, então

$$
\mathbb{E}({X}\hat\beta|{X}) = {X} \mathbb{E}(\hat\beta | {X}) = {X} \beta
$$

Portanto,

$$
\mathbb{E}(\hat{u}|{X}) 
= \mathbb{E}(Y|{X}) - \mathbb{E}(\hat Y|{X}) 
= {X} \beta - {X} \beta = 0
$$

Resumindo:

$$
\mathbb{E}(\hat{u}|{X}) = \mathbb{E}(Y-\hat Y|{X}) 
= \mathbb{E}(Y|{X}) - \mathbb{E}(\hat Y|{X}) 
= \mathbb{E}({X}\beta + u | {X} ) - \mathbb{E}({X}\hat\beta|{X}) 
$$
$$
= \mathbb{E}({X}\beta|{X}) + \mathbb{E}(u|{X}) - \mathbb{E}({X} \hat\beta|{X}) 
= {X}\beta + 0 - {X} \mathbb{E}(\hat\beta) 
= {X} \beta - {X} \beta = 0
$$

3.  Mostre que ${X^\prime} \hat{u} = 0$ e $\sum\limits_{i=1}^n \hat{u_i} = 0$

Da questão 1a) temos que

$$
\hat{\beta} = ({X}^\prime{X})^{-1} {X}^\prime{Y}
$$

Então $\hat{Y}$ pode ser escrito como

$$
\hat{Y} = {X} \hat{\beta} 
= {X} ({X}^\prime{X})^{-1} {X}^\prime{Y}
$$

Além disso, sabemos que $\hat{u} = Y -\hat Y = Y - {X}\hat{\beta}$. Dessa forma, temos que 

$$
{X}^\prime \hat{u} = {X}^\prime(Y - \hat{Y}) 
= {X}^\prime(Y - {X}\hat{\beta})
= {X}^\prime(Y - {X} ({X}^\prime{X})^{-1} {X}^\prime Y)
= {X}^\prime Y - {X}^\prime {X} ({X}^\prime{X})^{-1} {X}^\prime Y
$$
$$
= {X}^\prime Y - {I} {X}^\prime Y
= {X}^\prime Y - {X}^\prime Y = 0
$$

Agora, veja que $\sum\limits_{i=1}^n \hat{u_i}$ pode ser reescrito como $\boldsymbol{1}^\prime \hat{u}$, onde $\boldsymbol{1}$ é o vetor coluna de tamanho $n$ com todos os elementos iguais a 1. Dessa forma, lembrando que $\hat{u} = Y - \hat Y = Y - {X}\hat{\beta} = Y - {X} ({X}^\prime{X})^{-1} {X}^\prime {Y}$, temos que 

$$
\boldsymbol{1}' \hat{u} = \boldsymbol{1}' (Y - X (X'X)^{-1} X' Y) = \boldsymbol{1}'Y - \boldsymbol{1}'X (X'X)^{-1} X' Y
$$

Mas a matriz $X(X'X)^{-1}X'$ é uma matriz de projeção, isto é, $(X(X'X)^{-1}X')^2 = X(X'X)^{-1}X'$. Veja:

$$
X(X'X)^{-1}X' X(X'X)^{-1}X' = XI(X'X)^{-1}X' = X(X'X)^{-1}X'
$$

Isso implica que, qualquer vetor que já esteja no subespaço sobre o qual $X(X'X)^{-1}X'$ projeta, permanecerá inalterado após a aplicação da projeção, ou seja, $\boldsymbol{1}'X (X'X)^{-1} X' = \boldsymbol{1}^\prime$, pois $1$ está no espaço coluna de $\boldsymbol{X}$ (coluna de 1's referente ao intercepto). Portanto,

$$
\boldsymbol{1}' \hat{u} = \boldsymbol{1}' (Y - X (X'X)^{-1} X' Y) = \boldsymbol{1}'Y - \boldsymbol{1}'X (X'X)^{-1} X' Y = \boldsymbol{1}'Y - \boldsymbol{1}'Y = 0
$$


4.  Seja um modelo de regressão linear simples $Y = \beta_0 + \beta_1 + u$. Mostre que se a correlação amostral entre $Y$ e $X$ for positiva, então $\hat{\beta_1} > 0$. O que acontece com $\hat{\beta_1}$ se a correlação amostral entre $Y$ e $X$ for negativa? ($\hat{\beta_1}$ é o estimador MQO).

Como $\hat{\beta}_{1}$ é o estimador MQO, ele é dado por:

$$
\hat{\beta}_{1} = \frac{\sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^{n} (X_i - \bar{X})^2}
$$

E sabemos que a correlação amostral $r_{XY}$ é dada por:

$$
r_{XY} = \frac{Cov(X,Y)}{\sqrt{\mathbb{V}(X)\mathbb{V}(Y)}} 
= \frac{\sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum_{i=1}^{n} (X_i - \bar{X})^2 \sum_{i=1}^{n} (Y_i - \bar{Y})^2}} 
$$

E pode ser reescrita como

$$
\frac{\sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum_{i=1}^{n} (X_i - \bar{X})^2} \sqrt{\sum_{i=1}^{n} (Y_i - \bar{Y})^2}} \frac{\sqrt{\sum_{i=1}^{n} (X_i - \bar{X})^2}}{\sqrt{\sum_{i=1}^{n} (X_i - \bar{X})^2}}
= 
\frac{\sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^{n} (X_i - \bar{X})^2}\frac{\sqrt{\sum_{i=1}^{n} (X_i - \bar{X})^2}}{\sqrt{\sum_{i=1}^{n} (Y_i - \bar{Y})^2}}
$$

Portanto podemos reescrever $\hat{\beta}_{1}$ em termos da correlação amostral:

$$
\hat{\beta}_{1} = r_{XY} \frac{\sqrt{\sum_{i=1}^{n} (Y_i - \bar{Y})^2}}{\sqrt{\sum_{i=1}^{n} (X_i - \bar{X})^2}}
$$

Como $\frac{\sqrt{\sum_{i=1}^{n} (Y_i - \bar{Y})^2}}{\sqrt{\sum_{i=1}^{n} (X_i - \bar{X})^2}}$ é uma razão de somas de quadrados, esse termo é sempre positivo.

Portanto quando $r_{XY} > 0$ temos que $\hat{\beta}_{1} > 0$, e quando $r_{XY} < 0$ temos que $\hat{\beta}_{1} < 0$.


5.  [C2Q4] O conjunto de dados $\textit{BWGHT}$ contêm informações de nascimentos nos Estados Unidos. As duas variáveis de interesse são $\textit{bwght}$ (peso do recém-nascido em onças) e $\textit{cigs}$ (número médio de cigarros que a mãe fumou por dia durante a gravidez). Regrida $\textit{bwght}$ sobre $\textit{cigs}$ e responda às seguintes perguntas:

a)  Qual é o peso do nascimento previsto quando $\textit{cigs} = 0$? E quando $\textit{cigs} = 20$?

```{r, warning=FALSE}
library(wooldridge)

dados = as.data.frame(wooldridge::bwght)
modelo = lm(bwght ~ cigs, dados)

novos_dados = data.frame(cigs = c(0, 20))
previsoes = predict(modelo, novos_dados)

cat("O peso previsto do recém-nascido quando cigs = 0 é", 
    round(previsoes[1],2), "e quando cigs = 20 é", round(previsoes[2],2), "\n")
```

```{python}
import wooldridge
import statsmodels.api as sm
import pandas as pd

dados = wooldridge.data("bwght")
preditoras = sm.add_constant(dados.cigs) 
modelo = sm.OLS(dados.bwght, preditoras).fit()

novos_dados = sm.add_constant(pd.DataFrame({'cigs': [0, 20]}))
previsoes = modelo.predict(novos_dados)

print("O peso previsto do recém-nascido quando cigs = 0 é", 
      round(previsoes[0],2), "e quando cigs = 20 é", round(previsoes[1],2))
```

```{julia, results = "hide"}
using WooldridgeDatasets
using DataFrames
using GLM

dados = DataFrame(wooldridge("bwght"))

modelo = lm(@formula(bwght ~ cigs), dados)

novos_dados = DataFrame(cigs = [0, 20])
previsoes = predict(modelo, novos_dados)
```

```{julia}
println("O peso previsto do recém-nascido quando cigs = 0 é ", 
        round(previsoes[1], digits=2),
        " e quando cigs = 20 é ", round(previsoes[2], digits=2))
```

b)  O $\textit{MRLS}$ necessariamente captura uma relação causal entre o peso do nascimento da criança e os hábitos de fumar da mãe?

Não. Provavelmente existem outros fatores que influenciam no peso do recém-nascido que não foram incluídos no modelo.

c)  Para prever um peso de nascimento de 125 onças, qual deveria ser a magnitude de $\textit{cigs}$?

```{r}
cigs_calculado = (125 - modelo$coefficients[[1]])/modelo$coefficients[[2]]

cat("Para prever um peso de nascimento de 125 onças,",
"a magnitude de cigs deveria ser aproximadamente", 
round(cigs_calculado,2), "\n")

prev = predict(modelo, data.frame(cigs = c(cigs_calculado)))

cat("Veja que de fato quando cigs =", round(cigs_calculado,2), 
"o peso previsto do recém-nascido é", paste0(prev, "."),
"Entretanto, fumar um número negativo de cigarros não é possível.", "\n")
```

```{python}
cigs_calculado = (125 - modelo.params.iloc[0]) / modelo.params.iloc[1]

print("Para prever um peso de nascimento de 125 onças,",
"a magnitude de cigs deveria ser aproximadamente", 
round(cigs_calculado,2))

prev = modelo.predict([1, cigs_calculado])

print(f"Veja que de fato quando cigs = {round(cigs_calculado, 2)}, "
      f"o peso previsto do recém-nascido é {prev[0]:.2f}. "
      "Entretanto, fumar um número negativo de cigarros não é possível.")
```

```{julia, results = "hide"}
cigs_calculado = (125 - coef(modelo)[1]) / coef(modelo)[2]

println("Para prever um peso de nascimento de 125 onças,",
"a magnitude de cigs deveria ser aproximadamente ", 
round(cigs_calculado, digits=2))

prev = predict(modelo, DataFrame(cigs = cigs_calculado))
```

```{julia}
println("Veja que de fato quando cigs = ", round(cigs_calculado, digits=2), 
" o peso previsto do recém-nascido é ", prev[1], ". Entretanto, fumar um número",
" negativo de cigarros não é possível.")
```

d)  Verifique qual a proporção de mulheres que não fumaram durante a gravidez na amostra. Sua conclusão no item anterior muda?

```{r, message=FALSE}
library(tidyverse)

cigs0 = dados %>% filter(cigs == 0) %>% nrow()
prop = (cigs0 / nrow(dados)) * 100

cat("A proporção de mulheres na amostra que não fumaram durante a gravidez é de", 
paste0(round(prop,2), "%."), "Dessa forma, a conclusão no item anterior muda, pois",
"o ajuste do modelo é muito influenciado pela grande quantidade de mulheres que", 
"não fumaram durante a gravidez.")
```

```{python}
cigs0 = dados[dados.cigs == 0].shape[0]
prop = (cigs0 / dados.shape[0]) * 100

print("A proporção de mulheres na amostra que não fumaram durante a gravidez é "
      f"de {round(prop, 2)}%. Dessa forma, a conclusão no item anterior muda, "
      "pois o ajuste do modelo é muito influenciado pela grande quantidade de "
      "mulheres que não fumaram durante a gravidez.")
```

```{julia, results = "hide"}
cigs0 = nrow(filter(row -> row.cigs == 0, dados))
prop = (cigs0 / nrow(dados)) * 100
```

```{julia}
println("A proporção de mulheres na amostra que não fumaram durante a gravidez é de ", 
round(prop, digits=2), "%. ", "Dessa forma, a conclusão no item anterior muda, pois ",
"o ajuste do modelo é muito influenciado pela grande quantidade de mulheres que ", 
"não fumaram durante a gravidez.")
```

